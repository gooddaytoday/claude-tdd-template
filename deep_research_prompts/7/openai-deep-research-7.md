# Evaluation и continuous improvement для TDD-ориентированного agent harness

## Что именно оцениваем и почему это системная задача

Текущий harness в репозитории на entity["company","GitHub","code hosting platform"] уже устроен как «мини-продакшен» для агентной разработки: он принудительно проводит фичи через 6 фаз (RED → GREEN → REFACTOR → CODE REVIEW → ARCH REVIEW → DOCUMENTATION), требует независимой верификации прогонов тестов оркестратором и использует формализованные контракты (Context Packet, Phase Packet) вместо свободного пересказа контекста. citeturn9view0turn10view2turn11view0

Из этого следует ключевая рамка для eval-подхода: оценкой должен быть не «насколько хорошо отвечает модель», а насколько надёжно работает **система** (агенты + хуки + permissions + протоколы фаз) на реальных задачах. entity["company","Anthropic","ai research company"] подчёркивает, что для агентных систем оценка сложнее именно из‑за многотуровости, инструментов и изменения состояния окружения: важно мерить не только текстовый ответ, а «исход» (outcome) в среде, и фиксировать полную трассу (transcript/trace) попытки. citeturn16view0

В вашем harness уже заложены два мощных «объективных грейдера», которые стоит сделать центральными в eval framework:  
1) **гейты по тестам** (RED обязан падать «правильно», GREEN обязан проходить, REFACTOR обязан оставаться зелёным) с независимым запуском тест-команды самим оркестратором; citeturn9view0turn11view2  
2) **жёсткий TDD Guard** — PreToolUse-хук блокирует любые попытки модификации `tests/**` не теми ролями, и политика хранится как «policy-as-data». citeturn8view0turn10view0

Отдельно важна совместимость среды: Claude Code использует `.claude/settings.json` (хуки, permissions, env) и markdown‑субагентов; документация описывает иерархию конфигураций, deny‑паттерны для чувствительных файлов и общий принцип «инструкции через CLAUDE.md + настройки через JSON». citeturn8view2turn23view1turn23view0turn24view1  
Для Cursor, официальная страница документации по правилам в этой среде не открылась через текущий web‑рендерер, поэтому ниже опираюсь на доступные публичные источники: формат `.cursor/rules/*.mdc` с frontmatter и glob‑автоподключением, а также практики тестирования/применения правил через ручной вызов. citeturn15view0turn15view2turn15view3

## Метрики по ролям для шести субагентов

Ролевые метрики должны быть «привязаны к контрактам» (Phase Packet/Context Packet) и к тем ограничениям, которые уже явно прописаны в определениях субагентов (например: test‑writer не пишет имплементацию; implementer/refactorer не трогают тесты; reviewer не правит код напрямую; architect/documenter обязаны ходить в Task Master MCP). citeturn10view2turn11view0turn25view0turn25view1turn25view2turn25view3turn26view0turn26view1

Ниже — метрики уровня «минимально достаточные», которые (а) проверяемы автоматически, (б) различают качество/скорость/стабильность, (в) не поощряют «игру в метрику» (например, нельзя улучшить показатель, просто переписав тест). Общая логика соответствует рекомендациям: строить eval как набор задач с грейдерами, запускать несколько trials из‑за вариативности и оценивать результат, а не строго заданный путь действий. citeturn16view0

| Субагент | Главная ответственность (как в конфиге) | Основные метрики качества | Метрики скорости/стоимости | Метрики стабильности/надёжности |
|---|---|---|---|---|
| tdd-test-writer | Написать **падающие** unit/integration тесты и подтвердить падение | **RED Gate Purity**: падение из‑за assertion/ожидаемого поведения (а не синтаксис/импорты); **Spec fidelity**: тесты описывают поведение (Given/When/Then) и контракт экспорта (Contract surface); **Flake risk**: отсутствие скрытых источников недетерминизма | Time-to-RED; число итераций до «валидного падения» | Повторяемость падения при повторном прогоне одной командой; доля случаев, где тест случайно «зелёный» и требует перезапуска RED |
| tdd-implementer | Минимальная реализация, чтобы сделать тест зелёным, **не трогая тесты** | **Green Minimality**: объём изменений по сути минимален относительно требований теста (proxy: diff size + отсутствие несвязанных файлов); **Contract compliance**: реализованы именно заявленные exports/ошибки | Time-to-GREEN; число циклов «поправить код → прогнать тест» | Доля случаев «test still fails after 3 retries» (эскалации); доля «сломал соседние тесты» |
| tdd-refactorer | Улучшить качество, не меняя поведение, тесты остаются зелёными | **Refactor Safety**: тесты зелёные после каждого шага; **Delta value**: рефакторинг реально уменьшает сложность/дублирование или аргументированно фиксирует «no refactoring needed» | Time-to-REFACTOR; число прогонов тестов в фазе | Частота регрессий после рефакторинга (тесты краснеют → rollback/эскалация) |
| tdd-code-reviewer | Найти critical/major проблемы и оформить FixRequest, не правя код напрямую | **Issue precision**: доля «ложных блокеров» (critical/major, которые не воспроизводятся или не важны); **FixRequest quality**: полнота (file/location/severity/category/proposedFix/verificationCommand/routeTo) | Time-to-Review; количество fix-routing циклов до passed | Стабильность: одинаковая версия кода → схожий набор FixRequest (низкая «дрейфующая строгость») |
| tdd-architect-reviewer | Проверить интеграцию и структуру, найти orphaned code, при необходимости создать integration subtask | **Integration coverage**: обнаружение «висит в воздухе» (не импортится/не регистрируется); **Task context discipline**: факт получения parent+subtask из MCP | Time-to-ArchReview; время на Full Task Review на последнем сабтаске | Частота «ложных orphan» vs «пропущенных orphan» (по итогам интеграционных багов) |
| tdd-documenter | Сохранить детали в task-master и обновить CLAUDE.md по правилам | **Doc correctness**: `modifiedFiles` заполнен и структурирован; **Module doc completeness** на последнем сабтаске; **No-code-touch** соблюдён | Time-to-Docs | Доля случаев, где следующая итерация ломается из‑за неполной документации/отсутствия связок (proxy: количество «need context» вопросов в следующих сабтасках) |

Таблица выше «приземляется» на ваши существующие контракты. Например: Phase Packet формализует, какие фрагменты лога должны вернуться после RED/GREEN/REFACTOR и как выглядят FixRequest блоки; Context Packet требует, чтобы между фазами переносился единый контекст без дрейфа; а сами субагенты содержат обязательные ограничения, которые легко превратить в проверяемые compliance‑сигналы. citeturn10view2turn11view0turn25view0turn25view1turn25view2turn25view3turn26view0turn26view1

Практически важно различать **качество фазы** и **качество результата системы**: например, code-reviewer может «правильно» найти проблемы, но если fix-routing часто ломает тесты или уходит в циклы, это уже pipeline‑проблема, а не проблема одного агента. Такой взгляд соответствует агентной eval‑логике из инженерных рекомендаций: задача → несколько trial → grader(ы) → outcome. citeturn16view0turn11view2

## Сквозные KPI для TDD цикла и outcome-based grading

### Как мерить end-to-end качество, а не «локальный успех» фазы

Главный принцип из агентных eval‑подходов: оценивать **то, что произведено**, а не «идеальную последовательность действий». Слишком жёсткая проверка пути делает тесты хрупкими; лучше мерить outcome + частичный успех, особенно когда задача многокомпонентная. citeturn16view0

В вашем TDD skill уже зафиксировано, что Phase Packets — это «summary», а источником истины на гейтах является независимый запуск тестов оркестратором. Это ровно то, что Anthropic описывает как разницу между «текстом агента» и «outcome в среде». Поэтому для pipeline KPI разумно считать базовой истиной: **результаты `npm run test...` + состояние кода/файлов**, а не декларации субагентов. citeturn9view0turn16view0

### Рекомендуемый набор KPI уровня pipeline

Ниже — KPI, которые меряют систему «целиком», включая защитные механизмы и исправления после ревью. Они согласуются с вашим state machine (retry‑переходы, fix-routing циклы) и с типовыми failure‑режимами длинных агентных циклов (попытка сделать слишком много, преждевременное «готово», отсутствие реального тестирования). citeturn9view0turn16view1turn11view2

**KPI успешности (качество результата):**

- **Cycle Completion Rate**: доля сабтасков, завершённых с состоянием DONE без ручных вмешательств (кроме разрешённых `ask`). Основа — ваш workflow: DOCS → DONE после успешных CODE/ARCH review и сохранения документации. citeturn9view0turn26view1  
- **Gate Integrity Rate**: доля циклов, где RED действительно «красный» по требованиям (assertion‑ошибка), GREEN — действительно зелёный, REFACTOR — сохранил зелёный. Это прямо прописано как обязательный протокол в skill и pre-phase/phase документах. citeturn9view0turn11view2turn11view4  
- **Integration Health**: доля циклов без orphaned компонентов (или доля, где orphan корректно выявлен и оформлен integration subtask на последнем сабтаске). Это вытекает из контракта architect-reviewer и Phase Packet. citeturn26view0turn10view2  
- **Documentation Completeness**: доля циклов, где task-master update содержит `modifiedFiles` и где на последнем сабтаске обновлены module CLAUDE.md и root‑ссылки. citeturn26view1turn24view1turn10view2  

**KPI эффективности (скорость/стоимость):**

- **Median Time per Phase** и **P95 Cycle Time**: время RED→DONE по логам событий (минимально — timestamps Phase Packets; лучше — instrumentation hook’ов). Подход поддерживается тем, что Claude Code в принципе позволяет измерять длительность «ходов» и автономной работы по сессиям. citeturn20view0turn10view2  
- **Fix-routing Cost**: среднее число циклов «review → fix → tests → review», и доля эскалаций по лимиту (base 3, до 5 при прогрессе). Это уже формализовано в фазе CODE REVIEW и в skill. citeturn11view2turn9view0  
- **Tool Call Footprint**: количество Read/Glob/Grep/Write/Edit/Bash на цикл и на фазу. Это хороший прокси к стоимости и к «контекстной перегрузке», учитывая идею, что контекст — конечный ресурс, и его нужно держать «высокосигнальным и компактным». citeturn13view2turn20view0turn23view0  

**KPI надёжности (стабильность и безопасность):**

- **Guard Violation Attempts**: сколько раз за цикл поступали запрещённые попытки редактировать `tests/**` из неразрешённых ролей (deny‑события) и сколько было «подозрительных» паттернов отключения тестов (ask‑события по семантическим паттернам). Это прямо следует из guard hook и policy. citeturn8view0turn10view0  
- **Sandbox Boundary Escapes / Permission Friction**: сколько «ask» срабатываний по критическим командам или файлам (например git push/rebase/merge), и как это влияет на cycle time. Общая идея: меньше «approval fatigue» и лучше безопасность достигаются через чёткие границы доступа, а не бесконечные подтверждения. citeturn8view2turn19view1turn23view1  

### Как формализовать graders, чтобы они были проверяемыми и «не ломались»

Если следовать предложенной структуре eval’а (task → trials → graders → outcome), то для вашего harness graders удобно разложить в три слоя: citeturn16view0turn9view0turn10view2

1) **Outcome graders** (самые важные): тесты/линтер/типизация/структурная интеграция. Они проверяют конечное состояние репозитория и соответствуют принципу «grade what produced». citeturn16view0turn9view0turn11view2  
2) **Protocol compliance graders**: соблюдение контракта Phase Packet (наличие обязательных полей, корректные статусы, наличие verificationCommand и т.д.) и соблюдение запретов ролей (например, implementer не должен менять тесты). citeturn10view2turn25view1turn10view0  
3) **Trajectory quality graders** (аккуратно): не «путь как единственно верный», а мягкие сигналы вроде количества ненужных файловых операций или выходов за scope сабтаска. Это помогает, но не должно стать brittle‑критерием. citeturn16view0turn11view0  

## A/B эксперименты конфигураций без искажения результатов

### Почему A/B для агентного harness сложнее, чем для «плохого/хорошего промпта»

Для агентных систем вариативность выше: один и тот же вход может порождать разные траектории, поэтому требуется несколько trial на одну задачу, иначе вы путаете шум с регрессией. Это прямо заложено в терминологии eval (trial как одна попытка; множественные прогоны ради устойчивого сигнала). citeturn16view0

Дополнительно, по мере усиления ограничений (permissions, хуки, sandboxing) изменяется не только «качество кода», но и профиль автономности/риска: сколько времени агент способен работать без вмешательства, сколько раз запрашивает разрешение, насколько часто прерывается. В исследованиях по автономности подчёркивается, что автономность — свойство развертывания (модель + пользователь + продукт), и её нужно отдельно наблюдать и логировать. citeturn20view0turn19view1

### Дизайн A/B для ваших изменений

Чтобы сравнивать prompts/models/permissions «честно» и не искажать результаты, полезно стандартизировать эксперимент как мини‑eval harness:

**Единица сравнения**: «одна сабтаск‑задача» (или синтетическая задача) → полный прогон 6 фаз → финальный outcome. Это естественно согласуется с вашим state machine и с тем, что архитектурный Full Task Review выполняется на последнем сабтаске. citeturn9view0turn26view0

**Изоляция окружения**: каждый trial должен стартовать с идентичного состояния репозитория (fresh checkout/clean working tree), иначе вы подмешиваете эффекты «предыдущих шагов». Такой подход напрямую следует из концепции outcome и того, что агенты меняют состояние среды (файлы, зависимости, артефакты). citeturn16view0turn16view1

**Рандомизация и балансировка задач**:  
- сравнивать A и B на **одинаковых задачах** (paired design),  
- а порядок запусков перемешивать, чтобы не ловить эффекты «теплого кеша»/внешних факторов.  
Это особенно важно из‑за «контекстного износа» и ограниченного внимания моделей при длинных контекстах: даже небольшие отличия в контексте и истории могут менять поведение. citeturn13view2turn16view0

**Критерии успеха** — outcome‑ориентированные:  
- тесты проходят (и верно падают в RED),  
- нет нарушений guard и запретов,  
- review‑гейты пройдены,  
- документация сохранена.  
Это следует как из вашего skill протокола, так и из принципа «grade outcome, not path». citeturn9view0turn10view2turn16view0

### Что именно A/B‑тестировать в вашем harness

С практической точки зрения «варианты» стоит держать небольшими, иначе вы не узнаете, что именно сработало:

- **Prompts-инварианты**: изменения в формулировках subagent’ов и фазовых шаблонов (например, усиление формулировок вокруг минимальности GREEN или требований к FixRequest). Это логично, потому что субагенты в Claude Code воспринимают себя через описание роли и ограничения инструментов. citeturn23view0turn25view1turn10view2  
- **Модели по ролям**: замены модели для отдельных фаз (например, оставить более сильную модель на architecture review, а быстрые — на строго ограниченные операции), но мерить надо не «ощущения», а Cycle Completion Rate + Fix-routing Cost + качество outcome. citeturn23view0turn16view0  
- **Permissions/guardrails**: изменения allow/ask/deny паттернов и sandbox‑границ. Здесь outcome должен включать «безопасность» (меньше рискованных действий без подтверждения) и «продуктивность» (меньше approval fatigue). citeturn8view2turn19view1turn23view1  

### Как снижать «искажение из‑за промпт‑версий» и управлять версиями

В Anthropic документации по prompt tooling подчёркивается идея разделять фиксированную часть и переменные, что повышает тестируемость и упрощает версионирование, а evaluation tool — это ровно механизм «сравнивать версии промптов» системно. Даже если вы не используете их консоль напрямую, принцип полезен: фиксируйте версии инструкций как артефакт и прогоняйте на одинаковом наборе задач. citeturn22view0turn16view0

Практически для вашего репозитория это означает:  
- версионировать subagent’ов/skill/политики как «конфиг‑код» (что вы уже делаете), citeturn8view2turn9view0turn10view0  
- и добавлять к каждому эксперименту «манифест» (что именно изменилось) + сохранённые метрики по KPI (см. выше). Методологически это соответствует идее «eval harness как инфраструктура, которая запускает задачи и агрегирует результаты». citeturn16view0turn20view0

## Непрерывное улучшение: триггеры, базовая линия и регрессии

### Автоматические триггеры для prompt-refinement итераций

Anthropic отмечает, что без eval’ов команды попадают в реактивный цикл: чинят один фейл, ломают другое; а с eval‑подходом «провалы превращаются в тест‑кейсы» и развитие ускоряется, потому что появляется измеримый «холм, на который карабкаемся». citeturn16view0  
Эту идею можно сделать механикой harness’а: не ждать, пока человек заметит деградацию, а автоматически запускать refinement‑итерацию при конкретных сигналах.

Ниже — триггеры, которые хорошо «соответствуют архитектуре» вашего TDD цикла и уже существующим retry‑правилам:

**Триггеры качества (system health):**
- **Рост доли “RED invalid”**: тесты падают не assertion‑ошибкой, а импорты/синтаксис; значит промпт test-writer’а или шаблон теста провоцирует неверный старт. citeturn9view0turn25view0turn10view2  
- **Частые эскалации GREEN после 3 ретраев**: сигнал либо о неясном Contract surface (в RED), либо о недостаточной «минимальности и точности» GREEN‑инструкций. citeturn9view0turn25view1turn10view2  
- **Увеличение Fix-routing циклов** (уход к лимитам 3–5): вероятный признак того, что reviewer формирует FixRequest слишком расплывчато (низкое качество proposedFix/verificationCommand) или оркестратор часто misroute’ит (неправильный выбор implementer vs refactorer). citeturn11view2turn10view2turn9view0  

**Триггеры дисциплины и безопасности:**
- **Учащение Guard Violation Attempts**: агентная система «пытается» переписать тесты или отключить их семантически. Даже если технически это заблокировано, рост попыток — признак, что инструкции/контекст недостаточно ясно фиксируют «тесты священны». citeturn8view0turn10view0turn24view1  
- **Рост ask‑событий по опасным деревьям/командам** (git push/rebase/merge и т.п.): либо permissions слишком строги для вашего потока, либо workflow недостаточно структурирован и провоцирует лишние операции. Идея sandboxing в том, что правильно заданные границы сокращают approval prompts и улучшают безопасность одновременно. citeturn8view2turn19view1turn23view1  

**Триггеры контекста и управляемости:**
- **Рост Tool Call Footprint и “context bloat”**: симптом того, что агент «теряется» и ищет слишком широко. В терминах context engineering это означает, что контекст перестал быть «минимальным набором высокосигнальных токенов». citeturn13view2turn23view0turn20view0  

### Как организовать baseline benchmark и регрессионный контроль изменений harness

Чтобы изменения в harness не «ломали незаметно» качество, нужен базовый набор задач (benchmark suite), который отражает ваши типовые подзадачи. В терминах eval‑структуры: это коллекция tasks, для каждой из которых прогоняются trials, собираются transcripts и проверяется outcome. citeturn16view0turn20view0

Практический шаблон baseline suite для вашего TDD harness (примерная типология задач):
- «чистая логика» (unit‑доминирующая задача),  
- «интеграция с I/O» (integration‑доминирующая),  
- «оба слоя» (unit → integration),  
- «архитектурный хвост» (задача, где легко появится orphaned code и должна появиться integration subtask),  
- «документационный финал» (последний сабтаск, где обязательно обновление module CLAUDE.md и root‑индекса).  
Эта типология напрямую согласуется с вашим алгоритмом определения типа тестов и с обязанностями architect/documenter на последнем сабтаске. citeturn11view4turn10view2turn26view0turn26view1turn24view1

Для регрессивного контроля лучше использовать комбинацию «жёстких» и «мягких» порогов:

- **Жёсткие пороги** (CI‑блокеры): падение Cycle Completion Rate ниже baseline‑уровня; рост Gate Integrity ошибок; рост orphaned‑регрессий. Это outcome‑критерии, которые соответствуют принципу «мерить результат в среде». citeturn16view0turn9view0turn26view0  
- **Мягкие пороги** (warning): рост median времени фаз, рост количества tool calls, рост числа fix cycles, рост ask‑событий. Эти метрики важны, но сами по себе могут меняться, если задачи стали сложнее; поэтому их лучше страховать сравнением на paired tasks и несколькими trials. citeturn16view0turn20view0turn13view2  

Отдельная важная оговорка из практики eval framework: фреймворк/инфраструктура вторичны; качество определяется тем, **какие** задачи и грейдеры вы запускаете. Поэтому baseline suite должен эволюционировать: новые реальные провалы превращаются в новые задачи/грейдеры, иначе вы просто «подгоняете под старый экзамен». citeturn16view0

### Связка с Cursor: правила как часть continuous improvement

В Cursor‑экосистеме правила воспринимаются как «код»: их рекомендуют держать короткими, итеративно улучшать на основе болей и тестировать через явное подключение (например, ручным вызовом `@rule-name`). citeturn15view2turn15view3  
Формат rules‑файлов в сообществе описывается как `.cursor/rules/*.mdc` с frontmatter (description + globs), чтобы обеспечить авто‑подключение по матчингам файлов. citeturn15view0

Для вашей задачи это означает: если вы хотите сопоставимой дисциплины в CLI и IDE, то часть eval‑suite должна проверять не только outcome кода, но и то, что **инструкции/правила** действительно применяются «в нужных местах» (через globs/ручной вызов) и не конфликтуют с TDD Guard логикой в Claude Code. Это особенно важно, потому что Claude Code по своей архитектуре делает акцент на hooks/permissions и изолированное выполнение, а Cursor — на правила контекста и их подключение. citeturn8view2turn10view0turn15view0turn15view3turn23view1